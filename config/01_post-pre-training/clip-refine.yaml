pattern: CLIP-Refine
main: main/train.py
epoch: 1
batchsize: 512
experiment_iterations: 1
train_val_split_ratio: 0.99
snapshot_interval: 1
log_metrics: ["modality_gap", "loss_kd", "loss_feat"]

models:
  pattern: CLIP-ViT-B_32
  model:
    func: model/mod_clip.py
    name: CLIP
    args:
      backbone_name: "ViT-B/32"

dataset:
  dataset_func: data/coco_caption.py
  dataset_name: COCOCaption
  args:
    tokenizer: null
    transform: null
    test: False

dataset_cls:
  dataset_func: data/generic.py
  dataset_name: ImageNet
  use_ratio: 0.01
  classname_file: "data/classnames/imagenet.txt"
  args:
    transform: null
    test: False

optimizer:
  algorithm: AdamW
  args:
    lr: 1.0e-6
    weight_decay: 0.1
    
updater:
  func: updater/contrastive.py
  name: RandRegUpdater
  args:
    strategy: std_sample
    lambda_cont: 0.0
    lambda_rand: 1.0
    lambda_kd: 1.0
    distill_loss: kd
    alpha_blending: 0.5